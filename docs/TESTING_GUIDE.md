# ğŸ§ª Guide de Test du ModÃ¨le Fine-tunÃ©

Ce guide explique comment tester que ton modÃ¨le a Ã©tÃ© correctement fine-tunÃ©.

## ğŸ“‹ Table des matiÃ¨res

1. [PrÃ©-requis](#prÃ©-requis)
2. [VÃ©rification du Training](#vÃ©rification-du-training)
3. [Test d'InfÃ©rence Local](#test-dinfÃ©rence-local)
4. [Test via l'API](#test-via-lapi)
5. [MÃ©triques de QualitÃ©](#mÃ©triques-de-qualitÃ©)
6. [Troubleshooting](#troubleshooting)

---

## ğŸ”§ PrÃ©-requis

### Services Docker en cours d'exÃ©cution
```bash
cd /home/dorian/Documents/Epitech/HUB/Free-Project/Sienn-AI
docker compose up -d

# VÃ©rifier l'Ã©tat des services
docker ps --filter "name=sienn-"
```

Tous les services doivent Ãªtre **Up** et **healthy** :
- âœ… `sienn-api` (port 8000)
- âœ… `sienn-worker`
- âœ… `sienn-redis` (port 6379)
- âœ… `sienn-minio` (ports 9000-9001)
- âœ… `sienn-frontend` (port 3000)

---

## âœ… Ã‰tape 1: VÃ©rification du Training

### 1.1 Via le Frontend
1. Ouvre **http://localhost:3000**
2. Va dans la section **"ModÃ¨les"** ou **"Training Jobs"**
3. VÃ©rifie que ton job a le statut **"Completed"** âœ…

### 1.2 Via l'API
```bash
# Lister tous les jobs de training
curl http://localhost:8000/api/jobs

# VÃ©rifier un job spÃ©cifique
curl http://localhost:8000/api/training-status/<JOB_ID>
```

**RÃ©ponse attendue:**
```json
{
  "job_id": "xxx-xxx-xxx",
  "status": "completed",
  "progress": 100,
  "model_id": "yyy-yyy-yyy",
  "metrics": {
    "final_loss": 0.234,
    "training_time": "5m 23s"
  }
}
```

### 1.3 VÃ©rifier les fichiers du modÃ¨le
```bash
# Liste les modÃ¨les crÃ©Ã©s
ls -la data/models/

# VÃ©rifie le contenu d'un modÃ¨le spÃ©cifique
ls -la data/models/<MODEL_ID>/
```

**Fichiers attendus:**
- âœ… `adapter_config.json` - Configuration LoRA
- âœ… `adapter_model.safetensors` - Poids du modÃ¨le
- âœ… `README.md` - Documentation
- âœ… `training_args.bin` - Arguments d'entraÃ®nement

---

## ğŸ§ª Ã‰tape 2: Test d'InfÃ©rence Local

### Option A: Script Python Direct

```bash
# Depuis le dossier racine Sienn-AI
python tests/test_inference.py data/models/<MODEL_ID>
```

**Ce que fait ce script:**
- âœ… Charge le modÃ¨le GPT-2 de base
- âœ… Applique les adaptateurs LoRA
- âœ… GÃ©nÃ¨re des rÃ©ponses pour plusieurs prompts
- âœ… Compare avec le modÃ¨le de base

**Exemple de sortie:**
```
================================================================================
ğŸ§ª TEST D'INFÃ‰RENCE - ModÃ¨le: data/models/5bb4b302-612a-4463-885f-23e538ea9f2c
================================================================================

ğŸ“¦ Chargement du tokenizer...
ğŸ¤– Chargement du modÃ¨le de base GPT-2...
ğŸ”§ Chargement des adaptateurs LoRA...
ğŸ’» Utilisation du device: cuda

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Test #1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Prompt: Qu'est-ce que l'intelligence artificielle?

âœ… RÃ©ponse gÃ©nÃ©rÃ©e:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L'intelligence artificielle est la capacitÃ© des machines Ã  simuler...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Option B: Via Python REPL

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Charger
tokenizer = AutoTokenizer.from_pretrained("gpt2")
base_model = AutoModelForCausalLM.from_pretrained("gpt2")
model = PeftModel.from_pretrained(base_model, "data/models/<MODEL_ID>")

# Tester
prompt = "Qu'est-ce que l'IA?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

---

## ğŸŒ Ã‰tape 3: Test via l'API

### 3.1 Lister les modÃ¨les disponibles
```bash
python tests/test_api_inference.py
```

Ou avec curl:
```bash
curl http://localhost:8000/api/models | jq
```

### 3.2 Tester l'infÃ©rence via l'API
```bash
python tests/test_api_inference.py <MODEL_ID>
```

**Exemple:**
```bash
python tests/test_api_inference.py 5bb4b302-612a-4463-885f-23e538ea9f2c
```

### 3.3 Test manuel avec curl
```bash
curl -X POST http://localhost:8000/api/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "<MODEL_ID>",
    "prompt": "Explique-moi le machine learning",
    "max_length": 150,
    "temperature": 0.7
  }' | jq
```

**RÃ©ponse attendue:**
```json
{
  "generated_text": "Le machine learning est une branche...",
  "model_id": "xxx-xxx-xxx",
  "metadata": {
    "generation_time": "1.23s",
    "tokens_generated": 45
  }
}
```

### 3.4 Test via le Frontend
1. Ouvre **http://localhost:3000**
2. Va dans **"Inference"** ou **"Test Model"**
3. SÃ©lectionne ton modÃ¨le fine-tunÃ©
4. Entre un prompt de test
5. Clique sur **"Generate"**

---

## ğŸ“Š Ã‰tape 4: MÃ©triques de QualitÃ©

### 4.1 VÃ©rifier la perte (Loss)

La perte devrait **diminuer** pendant le training :

```bash
# Via l'API
curl http://localhost:8000/api/training-metrics/<JOB_ID> | jq '.metrics.loss'
```

**Valeurs attendues:**
- Loss initiale: ~3.0-5.0 (GPT-2)
- Loss finale: **< 1.0** (bon) ou **< 0.5** (excellent)

### 4.2 CohÃ©rence des RÃ©ponses

Compare les rÃ©ponses du modÃ¨le fine-tunÃ© avec le modÃ¨le de base:

```bash
python tests/test_inference.py <MODEL_ID>
# Appuie sur Enter pour voir la comparaison
```

**Attentes:**
- âœ… Le modÃ¨le fine-tunÃ© devrait donner des rÃ©ponses **plus pertinentes**
- âœ… Les rÃ©ponses devraient Ãªtre **dans le style** de ton dataset
- âœ… Moins de "hallucinations" ou de rÃ©ponses hors sujet

### 4.3 Temps de GÃ©nÃ©ration

```bash
# Mesure le temps de rÃ©ponse
time curl -X POST http://localhost:8000/api/inference \
  -H "Content-Type: application/json" \
  -d '{"model_id": "<MODEL_ID>", "prompt": "Test"}'
```

**Temps acceptable:**
- CPU: 1-5 secondes pour 50 tokens
- GPU: 0.1-0.5 secondes pour 50 tokens

---

## ğŸ¯ CritÃ¨res de SuccÃ¨s

Ton modÃ¨le est bien fine-tunÃ© si :

### âœ… CritÃ¨res Techniques
- [ ] Le training se termine avec `status: completed`
- [ ] La loss finale est < 1.0
- [ ] Les fichiers du modÃ¨le existent (`adapter_model.safetensors`, etc.)
- [ ] Le modÃ¨le charge sans erreur
- [ ] L'infÃ©rence gÃ©nÃ¨re du texte sans crash

### âœ… CritÃ¨res Qualitatifs
- [ ] Les rÃ©ponses sont **cohÃ©rentes** avec le prompt
- [ ] Le style correspond Ã  ton **dataset de training**
- [ ] Les rÃ©ponses sont **meilleures** que le modÃ¨le de base
- [ ] Pas de rÃ©pÃ©titions excessives ou de texte incohÃ©rent
- [ ] Le modÃ¨le "comprend" les instructions de ton dataset

---

## ğŸ” Tests Comparatifs

### Comparaison ModÃ¨le de Base vs Fine-tunÃ©

Utilise ces prompts basÃ©s sur ton `test_chat.csv`:

```python
test_prompts = [
    "Qu'est-ce que l'intelligence artificielle?",
    "Comment fonctionne le machine learning?",
    "Explique-moi les rÃ©seaux de neurones",
    "Quelle est la diffÃ©rence entre IA et ML?",
    "Comment dÃ©buter en data science?",
]
```

Pour chaque prompt, compare:

| CritÃ¨re | ModÃ¨le Base | ModÃ¨le Fine-tunÃ© | Note |
|---------|-------------|------------------|------|
| Pertinence | â­â­ | â­â­â­â­ | AmÃ©lioration |
| CohÃ©rence | â­â­â­ | â­â­â­â­â­ | Excellent |
| Style | GÃ©nÃ©rique | SpÃ©cialisÃ© | Conforme |
| Longueur | Variable | ContrÃ´lÃ©e | Bien |

---

## ğŸ› Troubleshooting

### âŒ Erreur: "Model not found"
```bash
# VÃ©rifie que le modÃ¨le existe
ls data/models/<MODEL_ID>

# VÃ©rifie dans la BDD
docker exec -it sienn-api python -c "
from app.db import get_db
from app.models import Model
db = next(get_db())
models = db.query(Model).all()
print([m.id for m in models])
"
```

### âŒ Erreur: "Out of memory"
- RÃ©duis `max_length` dans les requÃªtes d'infÃ©rence
- Utilise `batch_size=1`
- Active `use_cache=False`

### âŒ RÃ©ponses incohÃ©rentes
- Le modÃ¨le n'a peut-Ãªtre pas assez entraÃ®nÃ© (epochs trop faibles)
- Le dataset est trop petit (< 100 examples)
- Ajuste `temperature` (essaie 0.5 pour moins de variabilitÃ©)

### âŒ Training bloquÃ© en "Running"
```bash
# Check les logs du worker
docker logs sienn-worker --tail 50

# RedÃ©marre le worker
docker restart sienn-worker
```

---

## ğŸ“ Checklist ComplÃ¨te

Avant de valider ton modÃ¨le:

```
Phase 1: Training
[ ] Dataset uploadÃ© avec succÃ¨s
[ ] Training dÃ©marrÃ© sans erreur
[ ] Training terminÃ© avec status "completed"
[ ] Loss finale < 1.0
[ ] Fichiers du modÃ¨le prÃ©sents

Phase 2: Tests Techniques
[ ] Script test_inference.py passe sans erreur
[ ] Script test_api_inference.py passe sans erreur
[ ] L'API retourne des rÃ©ponses valides
[ ] Le frontend affiche le modÃ¨le

Phase 3: Tests Qualitatifs
[ ] RÃ©ponses cohÃ©rentes avec les prompts
[ ] Style conforme au dataset
[ ] AmÃ©lioration vs modÃ¨le de base
[ ] Temps de gÃ©nÃ©ration acceptable

Phase 4: Documentation
[ ] Screenshots des rÃ©sultats sauvegardÃ©s
[ ] MÃ©triques documentÃ©es
[ ] Prompts de test dÃ©finis
```

---

## ğŸš€ Prochaines Ã‰tapes

Une fois ton modÃ¨le validÃ©:

1. **Export** pour Ollama : `POST /api/export/<MODEL_ID>`
2. **Partage** : GÃ©nÃ©rer un README avec les performances
3. **Production** : DÃ©ployer l'API d'infÃ©rence
4. **AmÃ©lioration** : Fine-tune avec plus de donnÃ©es

---

## ğŸ“š Ressources

- [Documentation HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Guide LoRA/PEFT](https://huggingface.co/docs/peft)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

