"""
Script de test pour vÃ©rifier le modÃ¨le fine-tunÃ©
"""
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'backend'))

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

def test_model_inference(model_path: str, prompts: list[str]):
    """
    Teste l'infÃ©rence avec le modÃ¨le fine-tunÃ©
    
    Args:
        model_path: Chemin vers le modÃ¨le fine-tunÃ© (dossier avec adapter)
        prompts: Liste de prompts Ã  tester
    """
    print(f"\n{'='*80}")
    print(f"ğŸ§ª TEST D'INFÃ‰RENCE - ModÃ¨le: {model_path}")
    print(f"{'='*80}\n")
    
    try:
        # Charger le tokenizer
        print("ğŸ“¦ Chargement du tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        tokenizer.pad_token = tokenizer.eos_token
        
        # Charger le modÃ¨le de base
        print("ğŸ¤– Chargement du modÃ¨le de base GPT-2...")
        base_model = AutoModelForCausalLM.from_pretrained("gpt2")
        
        # Charger les adaptateurs LoRA
        print(f"ğŸ”§ Chargement des adaptateurs LoRA depuis {model_path}...")
        model = PeftModel.from_pretrained(base_model, model_path)
        model.eval()
        
        # Device
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        print(f"ğŸ’» Utilisation du device: {device}\n")
        
        # Tester chaque prompt
        for i, prompt in enumerate(prompts, 1):
            print(f"\n{'â”€'*80}")
            print(f"ğŸ¯ Test #{i}")
            print(f"{'â”€'*80}")
            print(f"ğŸ“ Prompt: {prompt}")
            print(f"\nğŸ¤” GÃ©nÃ©ration en cours...\n")
            
            # Tokenizer le prompt
            inputs = tokenizer(prompt, return_tensors="pt", padding=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # GÃ©nÃ©rer la rÃ©ponse
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    num_return_sequences=1,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # DÃ©coder la rÃ©ponse
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = generated_text[len(prompt):].strip()
            
            print(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e:")
            print(f"â”Œ{'â”€'*78}â”")
            print(f"â”‚ {response[:76]:<76} â”‚")
            if len(response) > 76:
                for j in range(76, len(response), 76):
                    print(f"â”‚ {response[j:j+76]:<76} â”‚")
            print(f"â””{'â”€'*78}â”˜")
        
        print(f"\n{'='*80}")
        print("âœ… Test d'infÃ©rence terminÃ© avec succÃ¨s!")
        print(f"{'='*80}\n")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ERREUR lors du test: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def compare_with_base_model(prompt: str, finetuned_model_path: str):
    """
    Compare les rÃ©ponses du modÃ¨le de base et du modÃ¨le fine-tunÃ©
    """
    print(f"\n{'='*80}")
    print(f"ğŸ” COMPARAISON: ModÃ¨le de base vs ModÃ¨le fine-tunÃ©")
    print(f"{'='*80}\n")
    print(f"ğŸ“ Prompt: {prompt}\n")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    
    # Test modÃ¨le de base
    print("ğŸ¤– RÃ©ponse du modÃ¨le de base GPT-2:")
    print("â”€" * 80)
    base_model = AutoModelForCausalLM.from_pretrained("gpt2").to(device)
    base_model.eval()
    
    inputs = tokenizer(prompt, return_tensors="pt", padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = base_model.generate(
            **inputs,
            max_new_tokens=100,
            num_return_sequences=1,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    base_response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()
    print(base_response)
    print()
    
    # Test modÃ¨le fine-tunÃ©
    print("ğŸ¯ RÃ©ponse du modÃ¨le fine-tunÃ©:")
    print("â”€" * 80)
    finetuned_model = PeftModel.from_pretrained(base_model, finetuned_model_path)
    finetuned_model.eval()
    
    with torch.no_grad():
        outputs = finetuned_model.generate(
            **inputs,
            max_new_tokens=100,
            num_return_sequences=1,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    finetuned_response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()
    print(finetuned_response)
    print()
    
    print(f"{'='*80}\n")


if __name__ == "__main__":
    # Exemples de prompts basÃ©s sur ton dataset test_chat.csv
    test_prompts = [
        "Qu'est-ce que l'intelligence artificielle?",
        "Comment fonctionne le machine learning?",
        "Explique-moi les rÃ©seaux de neurones",
        "Quelle est la diffÃ©rence entre IA et ML?",
        "Comment dÃ©buter en data science?",
    ]
    
    # Chemin vers ton modÃ¨le fine-tunÃ©
    # Remplace par l'ID de ton modÃ¨le aprÃ¨s le training
    model_path = "data/models/YOUR_MODEL_ID_HERE"
    
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
    
    if not os.path.exists(model_path):
        print(f"âŒ Erreur: Le modÃ¨le n'existe pas Ã  {model_path}")
        print("\nğŸ’¡ Utilisation:")
        print(f"   python {sys.argv[0]} <chemin_vers_modele>")
        print("\nExemple:")
        print(f"   python {sys.argv[0]} data/models/5bb4b302-612a-4463-885f-23e538ea9f2c")
        sys.exit(1)
    
    # Test d'infÃ©rence basique
    print("ğŸš€ DÃ©marrage des tests...\n")
    success = test_model_inference(model_path, test_prompts)
    
    # Comparaison avec le modÃ¨le de base
    if success:
        print("\n" + "="*80)
        input("Appuyez sur EntrÃ©e pour comparer avec le modÃ¨le de base...")
        compare_with_base_model(test_prompts[0], model_path)
    
    sys.exit(0 if success else 1)
