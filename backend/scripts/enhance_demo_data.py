#!/usr/bin/env python3
"""
Script pour enrichir les donn√©es de d√©mo avec des m√©triques d'entra√Ænement r√©alistes.
Ajoute des graphiques de loss, temps, et autres m√©triques pour rendre la d√©mo convaincante.
"""
import asyncio
import json
import sys
from datetime import datetime, timedelta
from pathlib import Path
from uuid import uuid4
import random

sys.path.insert(0, str(Path(__file__).parent.parent))

import aiosqlite
from app.core.config import settings


def generate_training_logs(num_epochs: int, base_loss: float = 2.0):
    """G√©n√®re des logs d'entra√Ænement r√©alistes"""
    logs = []
    current_loss = base_loss
    
    for epoch in range(1, num_epochs + 1):
        # Loss diminue progressivement avec un peu de bruit
        epoch_loss = current_loss * (0.7 + random.random() * 0.2)
        current_loss = epoch_loss
        
        # G√©n√©rer plusieurs steps par epoch
        steps_per_epoch = random.randint(15, 25)
        for step in range(1, steps_per_epoch + 1):
            step_loss = epoch_loss + random.uniform(-0.1, 0.1)
            logs.append({
                "epoch": epoch,
                "step": step,
                "loss": round(max(0.1, step_loss), 4),
                "learning_rate": 2e-4 * (0.95 ** (epoch - 1)),
                "timestamp": (datetime.now() - timedelta(hours=num_epochs - epoch, minutes=steps_per_epoch - step)).isoformat()
            })
    
    return logs


def generate_evaluation_metrics():
    """G√©n√®re des m√©triques d'√©valuation"""
    return {
        "accuracy": round(random.uniform(0.85, 0.95), 4),
        "perplexity": round(random.uniform(5, 15), 2),
        "bleu_score": round(random.uniform(0.65, 0.85), 4),
        "rouge_l": round(random.uniform(0.70, 0.90), 4),
        "training_samples": random.randint(100, 500),
        "validation_samples": random.randint(20, 100),
    }


async def enhance_job_metadata(conn, job_id: str, job_meta: dict):
    """Enrichit les m√©tadonn√©es d'un job avec des donn√©es r√©alistes"""
    
    num_epochs = job_meta.get("num_epochs", 3)
    
    # Ajouter des logs d'entra√Ænement
    training_logs = generate_training_logs(num_epochs, base_loss=job_meta.get("final_loss", 0.5) * 3)
    
    # Ajouter des m√©triques d'√©valuation
    eval_metrics = generate_evaluation_metrics()
    
    # Ajouter des d√©tails sur le dataset
    dataset_info = {
        "format": "csv",
        "columns": ["instruction", "response"],
        "examples_used": eval_metrics["training_samples"],
        "avg_prompt_length": random.randint(50, 200),
        "avg_response_length": random.randint(100, 400),
    }
    
    # Ajouter des infos sur les ressources utilis√©es
    resource_usage = {
        "peak_memory_gb": round(random.uniform(4, 12), 2),
        "avg_gpu_utilization": round(random.uniform(75, 95), 1),
        "total_training_time_seconds": random.randint(900, 3600),
        "avg_samples_per_second": round(random.uniform(2, 8), 2),
    }
    
    # Enrichir les m√©tadonn√©es existantes
    enhanced_meta = {
        **job_meta,
        "training_logs": training_logs,
        "evaluation_metrics": eval_metrics,
        "dataset_info": dataset_info,
        "resource_usage": resource_usage,
        "framework_version": {
            "transformers": "4.38.0",
            "peft": "0.9.0",
            "torch": "2.2.0",
        },
        "hyperparameters": {
            "lora_r": job_meta.get("lora_r", 16),
            "lora_alpha": job_meta.get("lora_alpha", 32),
            "lora_dropout": 0.05,
            "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],
            "optimizer": "adamw",
            "warmup_steps": 10,
            "gradient_accumulation_steps": 4,
        },
    }
    
    # Mettre √† jour dans la base de donn√©es
    await conn.execute(
        "UPDATE jobs SET meta = ? WHERE id = ?",
        (json.dumps(enhanced_meta), job_id)
    )
    
    return enhanced_meta


async def create_realistic_training_history():
    """Cr√©e un historique d'entra√Ænement r√©aliste pour tous les jobs completed"""
    
    print("üé® Enrichissement des donn√©es de d√©mo pour une pr√©sentation r√©aliste")
    print("=" * 80)
    
    db_path = Path(settings.database_path)
    conn = await aiosqlite.connect(str(db_path))
    
    try:
        # R√©cup√©rer tous les jobs completed
        cursor = await conn.execute(
            "SELECT id, meta FROM jobs WHERE status = 'completed'"
        )
        rows = await cursor.fetchall()
        
        print(f"\nüìä Enrichissement de {len(rows)} jobs completed...")
        print("-" * 80)
        
        for job_id, meta_str in rows:
            if not meta_str:
                continue
            
            meta = json.loads(meta_str)
            model_name = meta.get("model_name", "Unknown")
            
            print(f"\nüîß Traitement: {model_name}")
            print(f"   Job ID: {job_id}")
            
            # Enrichir les m√©tadonn√©es
            enhanced = await enhance_job_metadata(conn, job_id, meta)
            
            print(f"   ‚úÖ Ajout√©:")
            print(f"      - {len(enhanced['training_logs'])} training logs")
            print(f"      - M√©triques d'√©valuation (accuracy: {enhanced['evaluation_metrics']['accuracy']})")
            print(f"      - Dataset info ({enhanced['dataset_info']['examples_used']} exemples)")
            print(f"      - Resource usage (peak: {enhanced['resource_usage']['peak_memory_gb']} GB)")
        
        await conn.commit()
        
        print("\n" + "=" * 80)
        print("‚úÖ Enrichissement termin√© avec succ√®s!")
        print("\nüí° Ces jobs ont maintenant:")
        print("   - Graphiques de loss d√©taill√©s")
        print("   - M√©triques d'√©valuation")
        print("   - Utilisation des ressources")
        print("   - Logs d'entra√Ænement complets")
        print("\nüéØ Parfait pour votre pr√©sentation!")
        print("=" * 80)
        
    finally:
        await conn.close()


if __name__ == "__main__":
    asyncio.run(create_realistic_training_history())
